## Pretraining

Part of our pretraining data is sourced from open-source datasets. We thank [OpenUni](https://github.com/wusize/OpenUni) for providing well-organized data curation and download instructions, including:

- [megalith10m.md](datasets/megalith10m.md)
- [cc12m.md](datasets/cc12m.md)
- [text2image2m.md](datasets/text2image2m.md)
- [redcaps5m.md](datasets/redcaps5m.md)
- [laion6m.md](datasets/laion6m.md)

In addition, we introduce two high-quality datasets for pretraining:

- [midjourney_captioned_23m.md](datasets/midjourney_captioned_23m.md)
- [imagenet1k_t2i_qwenvl_flux.md](datasets/imagenet1k_t2i_qwenvl_flux.md)

---

## Finetuning

For finetuning, besides the datasets provided by [OpenUni](https://github.com/wusize/OpenUni):

- [text2image2m.md](datasets/text2image2m.md)
- [blip3o60k.md](datasets/blip3o60k.md)

we additionally include the **Echo-4o-Image** dataset:

- Dataset page: [Echo-4o-Image](https://huggingface.co/datasets/Yejy53/Echo-4o-Image)
- Download instructions: [echo_4o_image_100k.md](datasets/echo_4o_image_100k.md)